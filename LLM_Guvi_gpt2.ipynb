{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGOTgnOdR9Pd"
      },
      "source": [
        "#RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpxl8CavU8_V"
      },
      "source": [
        "##Preparing the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd9KvgwsaHOO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torchtext\n",
        "torchtext.disable_torchtext_deprecation_warning()\n",
        "\n",
        "# Sample text data\n",
        "text = \"\"\" Guvi Geek Networks Private Limited, commonly known as GUVI (an acronym for Grab Your Vernacular Imprint) , is an Indian technology company that focuses on providing skill development and training in information technology and computer science. GUVI was founded with the mission of bridging the skill gap in the IT industry by offering practical, industry-relevant training. The company caters to the growing demand for skilled professionals in emerging technologies like artificial intelligence, machine learning, data science, and programming.\n",
        "GUVI operates an online learning platform that offers courses, tutorials, and projects designed to enhance technical skills. It focuses on providing hands-on learning experiences through coding challenges, real-world projects, and interactive sessions. The platform offers courses covering programming languages such as Python, Java, C++, web development, mobile app development, data structures, algorithms, artificial intelligence, machine learning, blockchain, and cybersecurity.\n",
        "GUVI provides personalized learning solutions, including online learning, upskilling, and recruitment opportunities, emphasizing tech skills in native languages. The platform boasts over 3 million learners, 194 mentors, and 1673 educational videos across various topics and languages. It offers courses in multiple Indian languages, making technical education accessible to a diverse learner base.\n",
        "GUVI operates from its office in Chennai, India, where it manages its online learning platform and conducts various educational programs aimed at skill development in IT and related fields.\n",
        "Company type: Private Industry: E-Learning, Edtech, Online Education Founded: 2011 (as a YouTube channel), 2014 (as a company) Founders: Arun Prakash (founder & CEO), Late Sridevi Arun Prakash (co-founder), SP Balamurugan (co-founder) Website: guvi.in\n",
        "\"\"\"\n",
        "# Tokenize the text\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "tokens = tokenizer(text.lower())\n",
        "\n",
        "# Build the vocabulary\n",
        "counter = Counter(tokens)\n",
        "vocab = build_vocab_from_iterator([tokens], specials=['<unk>', '<pad>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "# Numericalize the text\n",
        "data = [vocab[token] for token in tokens]\n",
        "\n",
        "# Convert data to tensors and create batches\n",
        "seq_length = 30\n",
        "def create_batches(data, seq_length):\n",
        "    sequences = [data[i:i+seq_length] for i in range(0, len(data)-seq_length)]\n",
        "    inputs = torch.tensor([seq[:-1] for seq in sequences], dtype=torch.long)\n",
        "    targets = torch.tensor([seq[-1] for seq in sequences], dtype=torch.long)\n",
        "    return inputs, targets\n",
        "\n",
        "inputs, targets = create_batches(data, seq_length)\n",
        "train_data, val_data = inputs[:int(0.8*len(inputs))], inputs[int(0.8*len(inputs)):]\n",
        "train_targets, val_targets = targets[:int(0.8*len(targets))], targets[int(0.8*len(targets)):]\n",
        "\n",
        "# DataLoader\n",
        "train_dataset = torch.utils.data.TensorDataset(train_data, train_targets)\n",
        "val_dataset = torch.utils.data.TensorDataset(val_data, val_targets)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=20)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI5Zoh1rVRL6"
      },
      "source": [
        "##Implementing RNN Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7cnaGLmaKY1"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output\n",
        "\n",
        "# Initialize the model, criterion, and optimizer\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 100\n",
        "output_dim = vocab_size\n",
        "\n",
        "model = RNNModel(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaN77DqFVWdd"
      },
      "source": [
        "##Training the RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcrTJSunYWFB",
        "outputId": "4f6a6feb-0db1-41e2-92c8-b2a6048f82fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 5.6167, Val Loss: 5.6160\n",
            "Epoch 2, Train Loss: 4.8756, Val Loss: 5.7388\n",
            "Epoch 3, Train Loss: 4.2467, Val Loss: 5.6837\n",
            "Epoch 4, Train Loss: 3.7985, Val Loss: 5.7482\n",
            "Epoch 5, Train Loss: 3.3805, Val Loss: 5.6671\n",
            "Epoch 6, Train Loss: 2.9999, Val Loss: 5.6311\n",
            "Epoch 7, Train Loss: 2.6366, Val Loss: 5.6733\n",
            "Epoch 8, Train Loss: 2.3231, Val Loss: 5.5883\n",
            "Epoch 9, Train Loss: 2.0323, Val Loss: 5.6547\n",
            "Epoch 10, Train Loss: 1.7807, Val Loss: 5.7201\n",
            "Epoch 11, Train Loss: 1.5572, Val Loss: 5.7266\n",
            "Epoch 12, Train Loss: 1.3587, Val Loss: 5.7891\n",
            "Epoch 13, Train Loss: 1.1970, Val Loss: 5.7750\n",
            "Epoch 14, Train Loss: 1.0573, Val Loss: 5.8449\n"
          ]
        }
      ],
      "source": [
        "def train_epoch(loader, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, targets in loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(inputs)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate(loader, model, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            output = model(inputs)\n",
        "            loss = criterion(output, targets)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "for epoch in range(1, 15):\n",
        "    train_loss = train_epoch(train_loader, model, criterion, optimizer)\n",
        "    val_loss = evaluate(val_loader, model, criterion)\n",
        "    print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COLU8o7YaN48"
      },
      "source": [
        "##TESTING or generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cP9UyivYcq_",
        "outputId": "00fd462d-2da8-464f-864a-2110d7421b31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Guvi is known for  demand gap in indian languages such as python , machine , offering mentorship and 1673 educational videos across various topics and languages . it offers courses in multiple indian languages , making technical education accessible to a global audience , chennai , webkata , and 1673 educational videos across various\n"
          ]
        }
      ],
      "source": [
        "def generate_text(model, seed_text, vocab, tokenizer, next_words=50, temperature=1.0):\n",
        "    model.eval()\n",
        "    tokens = tokenizer(seed_text.lower())\n",
        "    input_ids = torch.tensor([vocab[token] for token in tokens], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    generated_text = seed_text\n",
        "    with torch.no_grad():\n",
        "        for _ in range(next_words):\n",
        "            output = model(input_ids)\n",
        "            output = output.squeeze(0)  # Remove the batch dimension\n",
        "            output = output / temperature\n",
        "            probabilities = torch.nn.functional.softmax(output, dim=-1)\n",
        "            next_token_id = torch.multinomial(probabilities, num_samples=1).item()\n",
        "            next_token = vocab.lookup_token(next_token_id)\n",
        "            generated_text += ' ' + next_token\n",
        "            next_input = torch.tensor([[next_token_id]], dtype=torch.long).to(device)\n",
        "            input_ids = torch.cat((input_ids, next_input), dim=1)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "seed_text = \"Guvi is known for \"\n",
        "generated_text = generate_text(model, seed_text, vocab, tokenizer, next_words=50, temperature=0.6)\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdYI5GGvbXBg"
      },
      "source": [
        "##LSTM On same data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVJA-IQtZmFc",
        "outputId": "306ad3be-247e-4c40-d5b7-89eefb807c65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 5.6292, Val Loss: 5.6615\n",
            "Epoch 2, Train Loss: 5.1052, Val Loss: 5.9225\n",
            "Epoch 3, Train Loss: 4.5564, Val Loss: 6.1674\n",
            "Epoch 4, Train Loss: 4.1673, Val Loss: 6.2267\n",
            "Epoch 5, Train Loss: 3.7889, Val Loss: 6.2097\n",
            "Epoch 6, Train Loss: 3.4181, Val Loss: 6.3458\n",
            "Epoch 7, Train Loss: 3.0692, Val Loss: 6.4432\n",
            "Epoch 8, Train Loss: 2.7833, Val Loss: 6.5030\n",
            "Epoch 9, Train Loss: 2.4978, Val Loss: 6.3939\n",
            "Epoch 10, Train Loss: 2.2495, Val Loss: 6.5381\n",
            "Epoch 11, Train Loss: 2.0036, Val Loss: 6.6106\n",
            "Epoch 12, Train Loss: 1.7798, Val Loss: 6.6885\n",
            "Epoch 13, Train Loss: 1.5835, Val Loss: 6.6330\n",
            "Epoch 14, Train Loss: 1.4191, Val Loss: 6.8067\n"
          ]
        }
      ],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output\n",
        "\n",
        "# Initialize the LSTM model\n",
        "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Train the LSTM model\n",
        "for epoch in range(1, 15):\n",
        "    train_loss = train_epoch(train_loader, model, criterion, optimizer)\n",
        "    val_loss = evaluate(val_loader, model, criterion)\n",
        "    print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55wwXpbDbJQ0",
        "outputId": "ad6f303e-1251-4a50-b1eb-a90d80f11b8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Guvi is  operates an an online for experts that on providing in information technology and computer science . guvi was founded with the mission of bridging the skill gap in the it industry by offering practical , industry-relevant training in indian languages , making technical education accessible to a global audience ,\n"
          ]
        }
      ],
      "source": [
        "def generate_text(model, seed_text, vocab, tokenizer, next_words=50, temperature=1.0):\n",
        "    model.eval()\n",
        "    tokens = tokenizer(seed_text.lower())\n",
        "    input_ids = torch.tensor([vocab[token] for token in tokens], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    generated_text = seed_text\n",
        "    with torch.no_grad():\n",
        "        for _ in range(next_words):\n",
        "            output = model(input_ids)\n",
        "            output = output.squeeze(0)  # Remove the batch dimension\n",
        "            output = output / temperature\n",
        "            probabilities = torch.nn.functional.softmax(output, dim=-1)\n",
        "            next_token_id = torch.multinomial(probabilities, num_samples=1).item()\n",
        "            next_token = vocab.lookup_token(next_token_id)\n",
        "            generated_text += ' ' + next_token\n",
        "            next_input = torch.tensor([[next_token_id]], dtype=torch.long).to(device)\n",
        "            input_ids = torch.cat((input_ids, next_input), dim=1)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "seed_text = \"Guvi is \"\n",
        "generated_text = generate_text(model, seed_text, vocab, tokenizer, next_words=50, temperature=0.5)\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWRcxSwAbdyQ"
      },
      "source": [
        "##TRANSFORMER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stN7K7ASbUdY",
        "outputId": "9f2be6e9-f59d-4675-d272-2e41caa77722"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, nhead, num_encoder_layers, hidden_dim, output_dim):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_encoder = PositionalEncoding(embedding_dim)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(embedding_dim, nhead, hidden_dim)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x) * math.sqrt(embedding_dim)\n",
        "        embedded = self.pos_encoder(embedded)\n",
        "        output = self.transformer_encoder(embedded)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output\n",
        "\n",
        "# Initialize the model, criterion, and optimizer\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 200\n",
        "nhead = 2\n",
        "num_encoder_layers = 2\n",
        "hidden_dim = 200\n",
        "output_dim = vocab_size\n",
        "\n",
        "model = TransformerModel(vocab_size, embedding_dim, nhead, num_encoder_layers, hidden_dim, output_dim).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJGoGlNLbqHH",
        "outputId": "1c1825fc-9cf5-4e22-ada4-da2b953348d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 5.3574, Val Loss: 5.6377\n",
            "Epoch 2, Train Loss: 4.2155, Val Loss: 5.7221\n",
            "Epoch 3, Train Loss: 3.4740, Val Loss: 5.8226\n",
            "Epoch 4, Train Loss: 2.8766, Val Loss: 5.8502\n",
            "Epoch 5, Train Loss: 2.4199, Val Loss: 5.9133\n",
            "Epoch 6, Train Loss: 2.0805, Val Loss: 6.1847\n",
            "Epoch 7, Train Loss: 1.8596, Val Loss: 6.2377\n",
            "Epoch 8, Train Loss: 1.7036, Val Loss: 6.5139\n",
            "Epoch 9, Train Loss: 1.6112, Val Loss: 6.5125\n",
            "Epoch 10, Train Loss: 1.5796, Val Loss: 6.5814\n",
            "Epoch 11, Train Loss: 1.5521, Val Loss: 6.5972\n",
            "Epoch 12, Train Loss: 1.5446, Val Loss: 6.6401\n",
            "Epoch 13, Train Loss: 1.4768, Val Loss: 6.8497\n",
            "Epoch 14, Train Loss: 1.4680, Val Loss: 7.0320\n"
          ]
        }
      ],
      "source": [
        "def train_epoch(loader, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, targets in loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(inputs)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate(loader, model, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            output = model(inputs)\n",
        "            loss = criterion(output, targets)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "for epoch in range(1, 15):\n",
        "    train_loss = train_epoch(train_loader, model, criterion, optimizer)\n",
        "    val_loss = evaluate(val_loader, model, criterion)\n",
        "    print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DPoxuFwcek4",
        "outputId": "9bf1a0e9-3042-4ea9-f7f2-be76358ac2b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Guvi is  an online learning , and career fairs , and career advancement . it focuses on providing skill development and career goals . it and career advancement . guvi . the mission of bridging the mission of bridging the skill development and supported by offering practical , and career advancement in\n"
          ]
        }
      ],
      "source": [
        "def generate_text(model, seed_text, vocab, tokenizer, next_words=50, temperature=0.5):\n",
        "    model.eval()\n",
        "    tokens = tokenizer(seed_text.lower())\n",
        "    input_ids = torch.tensor([vocab[token] for token in tokens], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    generated_text = seed_text\n",
        "    with torch.no_grad():\n",
        "        for _ in range(next_words):\n",
        "            output = model(input_ids)\n",
        "            output = output.squeeze(0)  # Remove the batch dimension\n",
        "            output = output / temperature\n",
        "            probabilities = torch.nn.functional.softmax(output, dim=-1)\n",
        "            next_token_id = torch.multinomial(probabilities, num_samples=1).item()\n",
        "            next_token = vocab.lookup_token(next_token_id)\n",
        "            generated_text += ' ' + next_token\n",
        "            next_input = torch.tensor([[next_token_id]], dtype=torch.long).to(device)\n",
        "            input_ids = torch.cat((input_ids, next_input), dim=1)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "seed_text = \"Guvi is \"\n",
        "generated_text = generate_text(model, seed_text, vocab, tokenizer, next_words=50, temperature=0.5)\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijE4ZMicA_o3"
      },
      "source": [
        "#GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO-psfetAjJT",
        "outputId": "2f0914d5-0e31-44a3-bf33-a3bc164a6ae4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "# Install the transformers library if not already installed\n",
        "!pip install transformers\n",
        "\n",
        "# Import the necessary library\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Login using the token\n",
        "login(token=\"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\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp0e3IhudhxH"
      },
      "source": [
        "#LLM finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W21bCkJqdhjN",
        "outputId": "bd355d83-7c24-4e2a-8514-5a77a2aabc25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.111.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.37.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.7.4)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.0.4)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.27.0)\n",
            "Requirement already satisfied: python-multipart>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.0.9)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from fastapi) (5.10.0)\n",
            "Requirement already satisfied: orjson>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from fastapi) (3.10.5)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.2.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi) (2.6.1)\n",
            "Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi) (3.7)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.10/dist-packages (from fastapi-cli>=0.0.2->fastapi) (0.12.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.22.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.0->fastapi) (1.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (13.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers datasets torch fastapi uvicorn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uF7qixThZlG"
      },
      "source": [
        "## data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmHwVuDFbdZ8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "def preprocess_data(input_file, output_file, tokenizer_name=\"gpt2\"):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_name)\n",
        "\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for line in lines:\n",
        "            # Convert the line to lowercase\n",
        "            line = line.lower()\n",
        "\n",
        "            # Tokenize the line\n",
        "            token_ids = tokenizer.encode(line, add_special_tokens=False)\n",
        "            # Convert token IDs back to tokens\n",
        "            tokenized_line = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "\n",
        "            # Rebuild the line from tokens, handle special characters\n",
        "            processed_line = \"\".join(\n",
        "                token.replace('Ä ', ' ').replace('ÄŠ', '').replace('ï¿½', '') for token in tokenized_line\n",
        "            ).strip()\n",
        "\n",
        "            # Remove special characters and unwanted tokens\n",
        "            processed_line = re.sub(r'[^a-zA-Z0-9\\s,():%&]', '', processed_line)\n",
        "            processed_line = re.sub(r'\\bgu vi\\b', 'guvi', processed_line)\n",
        "\n",
        "            # Remove extra spaces\n",
        "            processed_line = re.sub(r'\\s+', ' ', processed_line)\n",
        "\n",
        "            # Write the processed line to the output file\n",
        "            f.write(processed_line + \"\\n\")\n",
        "\n",
        "input_file = \"/content/Guvi_data.txt\"  # Your company-specific data file\n",
        "output_file = \"processed_guvi_data.txt\"\n",
        "preprocess_data(input_file, output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kmewyr8jfAGZ",
        "outputId": "c60a9441-95a3-4a05-ba67-fa4ec31e9e90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install accelerate -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2Qy1JYuhbxZ"
      },
      "source": [
        "##finetuneing the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "1xEolW4heHiP",
        "outputId": "57abe08e-9fbb-4acd-e9b0-04767e3a7d6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 08:00, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('./fine_tuned_model/tokenizer_config.json',\n",
              " './fine_tuned_model/special_tokens_map.json',\n",
              " './fine_tuned_model/vocab.json',\n",
              " './fine_tuned_model/merges.txt',\n",
              " './fine_tuned_model/added_tokens.json')"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Create dataset\n",
        "def load_dataset(file_path, tokenizer, block_size=128):\n",
        "    return TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=file_path,\n",
        "        block_size=block_size,\n",
        "    )\n",
        "train_dataset = load_dataset(output_file, tokenizer)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Initialize data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model and tokenizer\n",
        "model.save_pretrained(\"./fine_tuned_model\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H7qlpt7fpS_",
        "outputId": "0e323f14-4f71-4e42-cf5b-4ffca54348ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: accelerate\n",
            "Version: 0.31.0\n",
            "Summary: Accelerate\n",
            "Home-page: https://github.com/huggingface/accelerate\n",
            "Author: The HuggingFace team\n",
            "Author-email: zach.mueller@huggingface.co\n",
            "License: Apache\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db8zT_fshgke"
      },
      "source": [
        "##TESTING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4r9vK72fRMb",
        "outputId": "d901ab87-5f11-4e47-caa2-ef16ddc0f23c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text 1:\n",
            "guvi is a free, open source, open source, and collaborative platform for learning, development, and collaboration.\n",
            "\n",
            "Learn more about our mission\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#!pip install transformers\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_name_or_path = \"./fine_tuned_model\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name_or_path)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define the text generation function\n",
        "def generate_text(model, tokenizer, seed_text, max_length=100, temperature=1.0, num_return_sequences=1):\n",
        "    # Tokenize the input text\n",
        "    input_ids = tokenizer.encode(seed_text, return_tensors='pt').to(device)\n",
        "\n",
        "    # Generate text\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "        )\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_texts = []\n",
        "    for i in range(num_return_sequences):\n",
        "        generated_text = tokenizer.decode(output[i], skip_special_tokens=True)\n",
        "        generated_texts.append(generated_text)\n",
        "\n",
        "    return generated_texts\n",
        "\n",
        "# Test the model\n",
        "seed_text = \"guvi is\"\n",
        "generated_texts = generate_text(model, tokenizer, seed_text, max_length=30, temperature=0.01, num_return_sequences=1)\n",
        "\n",
        "for i, text in enumerate(generated_texts):\n",
        "    print(f\"Generated Text {i + 1}:\\n{text}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2AMNVxRfRiW",
        "outputId": "1a6aeccf-262e-445b-b530-e04049fe1fb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.111.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.30.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.1.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.37.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.7.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.0.4)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.27.0)\n",
            "Requirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (3.1.4)\n",
            "Requirement already satisfied: python-multipart>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.0.9)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from fastapi) (5.10.0)\n",
            "Requirement already satisfied: orjson>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from fastapi) (3.10.5)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.2.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi) (2.6.1)\n",
            "Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi) (3.7)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.10/dist-packages (from fastapi-cli>=0.0.2->fastapi) (0.12.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.2->fastapi) (2.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.18.4)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.22.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.0->fastapi) (1.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (13.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers fastapi uvicorn nest-asyncio pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYYkAVgAhldq"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, Request\n",
        "from pydantic import BaseModel\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_name_or_path = \"./fine_tuned_model\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name_or_path)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "class Query(BaseModel):\n",
        "    text: str\n",
        "\n",
        "@app.post(\"/generate/\")\n",
        "async def generate_text(query: Query):\n",
        "    input_ids = tokenizer.encode(query.text, return_tensors=\"pt\").to(device)\n",
        "    output = model.generate(input_ids, max_length=100, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95)\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return {\"generated_text\": generated_text}\n",
        "@app.get(\"/generatee/\")\n",
        "async def generate_text(text: str):\n",
        "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
        "    output = model.generate(input_ids, max_length=100, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95)\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return {\"generated_text\": generated_text}\n",
        "\n",
        "# Authenticate ngrok\n",
        "from pyngrok import conf, ngrok\n",
        "conf.get_default().auth_token = \"---------\" # enter your token\n",
        "\n",
        "# Start ngrok\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(\"Public URL:\", ngrok_tunnel.public_url)\n",
        "\n",
        "# Allow nested asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Run the FastAPI app\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7wIES28kXEq"
      },
      "source": [
        "https://422e-34-136-54-234.ngrok-free.app/generatee/?text=google%20is%20for"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjbTJRDEhudv"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Define the public URL and the endpoint\n",
        "public_url = \"https://a335-34-86-46-188.ngrok-free.app/\"\n",
        "endpoint = \"/generate/\"\n",
        "\n",
        "\n",
        "# Combine the URL and endpoint\n",
        "url = public_url + endpoint\n",
        "\n",
        "# Define the data to be sent in the POST request\n",
        "data = {\n",
        "    \"text\": \"Guvi is known for\"\n",
        "}\n",
        "\n",
        "try:\n",
        "    # Send the POST request\n",
        "    response = requests.post(url, json=data)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Attempt to parse JSON response\n",
        "        try:\n",
        "            json_response = response.json()\n",
        "            print(\"Response:\", json_response)\n",
        "        except json.JSONDecodeError as json_err:\n",
        "            print(f\"Failed to decode JSON response: {json_err}\")\n",
        "            print(\"Response text:\", response.text)\n",
        "    else:\n",
        "        print(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
        "\n",
        "except requests.RequestException as req_err:\n",
        "    print(f\"Request error: {req_err}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svr_0ee9XGQ3"
      },
      "source": [
        "#STEAMLIT APP ON THE LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7h1NP2bXPS1",
        "outputId": "142e1653-fa67-4080-e123-39f2736cdf7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.36.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (16.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.4.1)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<5,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP6r87FpjWhq",
        "outputId": "da109c81-b849-4a55-a646-0b00d8d9a514"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "#model_name_or_path = \"./fine_tuned_model\"\n",
        "model_name_or_path = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name_or_path)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define the text generation function\n",
        "def generate_text(model, tokenizer, seed_text, max_length=100, temperature=1.0, num_return_sequences=1):\n",
        "    input_ids = tokenizer.encode(seed_text, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "        )\n",
        "    generated_texts = [tokenizer.decode(output[i], skip_special_tokens=True) for i in range(num_return_sequences)]\n",
        "    return generated_texts\n",
        "\n",
        "# Streamlit app\n",
        "st.title(\"Text Generation with GPT-2\")\n",
        "st.write(\"This app generates text using a fine-tuned GPT-2 model. Enter a prompt and the model will generate a continuation.\")\n",
        "\n",
        "seed_text = st.text_input(\"Enter your prompt:\", \"Guvi is known for\")\n",
        "max_length = st.slider(\"Max Length:\", min_value=50, max_value=500, value=100)\n",
        "temperature = st.slider(\"Temperature:\", min_value=0.1, max_value=2.0, value=1.0)\n",
        "\n",
        "if st.button(\"Generate\"):\n",
        "    with st.spinner(\"Generating text...\"):\n",
        "        generated_texts = generate_text(model, tokenizer, seed_text, max_length, temperature)\n",
        "        for i, generated_text in enumerate(generated_texts):\n",
        "            st.subheader(f\"Generated Text {i + 1}\")\n",
        "            st.write(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zKJzlF6YKIU",
        "outputId": "37655cf9-e780-488d-954b-6326750877b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.1.6)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCSN2zoimQ73"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI39yMEDXMPv",
        "outputId": "327dee75-a274-4d2f-99a9-7093acc1db86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://42ba-34-86-46-188.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "Stopping Streamlit app...\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import conf, ngrok\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Authenticate ngrok\n",
        "conf.get_default().auth_token = \" ------- \" # enter your token\n",
        "\n",
        "# Run the Streamlit app in the background\n",
        "process = subprocess.Popen(['streamlit', 'run', 'app.py'])\n",
        "\n",
        "# Give the Streamlit app a few seconds to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Expose the Streamlit app to the web using ngrok\n",
        "public_url = ngrok.connect(addr=\"8501\")\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "# Keep the Colab cell running\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Stopping Streamlit app...\")\n",
        "    process.terminate()\n",
        "    ngrok.disconnect(public_url)\n",
        "    ngrok.kill()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyU3H2R5ZAcG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}